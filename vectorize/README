* About VECTORIZE

　　形態素解析結果からドキュメントのTF/IDF（特徴を表すベクトル）を算出します。

　　また、その途中経過を利用する事で、フルテキストサーチ（ANDやORなど）が可能になります。


* 用語

** TF (Term-Frequency)：　単語頻出度

　　TF = 該当単語がドキュメント内に何回現れたか
　　一般的に頻出単語程、そのドキュメント内で重要な単語と見做します。

** DF (Document-Frequency)：　文書頻出度

　　DF = 該当単語が幾つのドキュメントに現れたか？

** IDF (Inverse DF)：　逆文書頻出度

　　IDF = log( 総ドキュメント数 / DF )

　　多数のドキュメントに跨って出現する単語は、ドキュメントの特徴を表しているとは言えない。

　　代名詞（私、彼）、助詞／助動詞（は、です）などは、ほぼ全ドキュメントに出現し、
　　IDFが極小となるので評価から除外できる。

** TF-IDF

　　TF-IDF = TF x IDF

　　これを全単語に対して評価したもの（高次元ベクトル）を、ドキュメントの特徴と捉えている。


* 処理順
　　１．TF    : <= Token
　　２．DF    : <= TF
　　３．IDF   : <= DF
　　４．TFIDF : <= TF + IDF

* 高速化

　　TF-IDFはまともに全ての単語を評価するとベクトルの次元数が膨大になる為、ある程度間引きする必要がある。
　　
　　今回はIDFの段階で、閾値（threshold）と制限値（limit）でフィルタリングしている。
　　フィルタリングされた単語は、IDFコレクション中（"value"=0）とされ、TF-IDFでも0と判定され、除外される。

　　また、DF=1 （１つのドキュメントにだけ含まれている単語）はドキュメントの特徴を捉えているものの、
　　他ドキュメントと比較する術が無いのでやはり除外する。


* ドキュメント検索

　　TFを算出した段階で、ドキュメント検索が可能になる。（クイックスタート参照）


* クイックスタート

　　１．形態素解析まで行う

　　　https://github.com/monmo/monmo-NLProcessing/blob/master/tokenize/README

　　２．ベクトル算出

　　　./vectorize.sh -s test.token.sampledoc

　　３．文書検索

　　　./fulltext_search.sh -s test.vector.tf.token.sampledoc -w 'はさみや糊など' -V -L 1000
　　

* 処理手順
　　
　　１．TF
　　　./tf.sh    -s test.token.sampledoc            -o test.vector.tf.token.sampledoc
　　２．DF
　　　./df.sh    -s test.vector.tf.token.sampledoc  -o test.vector.df.token.sampledoc
　　３．IDF
　　　./idf.sh   -s test.vector.df.token.sampledoc  -o test.vector.idf.token.sampledoc
　　４．TF-IDF
　　　./tfidf.sh -s test.vector.idf.token.sampledoc -o test.vector.tfidf.token.sampledoc


　　IDFチューニング：

　　１．DFとIDFを確認する。

　　　./view_df.sh -s test.vector.df.token.sampledoc

　　　./view_df.sh -s test.vector.idf.token.sampledoc


　　２．上を確認しながら、limit,threshold,verb-onlyの値を調整する。

　　　limit:     （DF / 総ドキュメント数）の最大値
　　　threshold: DFの最小値
　　　verb-only: 名詞だけ抽出

　　　切り捨て過ぎの場合はlimit値を下げる：
　　　./idf.sh --limit 0.3   -s test.vector.df.token.sampledoc  -o test.vector.idf.token.sampledoc

　　　もっと切り捨てたい場合はlimit値を上げる：
　　　./idf.sh --limit 0.5   -s test.vector.df.token.sampledoc  -o test.vector.idf.token.sampledoc

　　　名詞だけを評価する：（大抵の場合、条件を緩くした方が良い）
　　　./idf.sh --limit 0.3 --verb-only -s test.vector.df.token.sampledoc  -o test.vector.idf.token.sampledoc


　　３．結果が良くなるまで繰り返す


　　４．TF-IDFを再産出
　　　./tfidf.sh -s test.vector.idf.token.sampledoc -o test.vector.tfidf.token.sampledoc

* TODO

　　差分解析を実装
　　　完全なTF/IDFではなくとも、リアルタイム性を得る為に全体解析せずに値を得たい。

　　　１．TFは普通にかける
　　　２．DF,IDFはスキップし、既存の結果を使いTFIDFを得る。（新規単語はIDF=0とし無視）

　　　頻出単語（ＤＦが大きい）では良い近似値が得られるが、反対に稀な単語は誤差が大きくなる。
　　　そういった単語はIDFが大きく、重要単語である事が難点。

　　　また初回解析でDF<=0の単語に至っては解析不能。

      == 検証 ==

                                              Doc1              Doc3000            Doc3001
                N       DF        IDF         TF    TF/IDF      TF     TF/IDF      TF     TF/IDF
初期ベクトル    3000    foo:10    foo:2.477   foo:0 foo:0       foo:1  foo:2.477   
                        bar:1000  bar:0.477   bar:5 bar:2.385   bar:10 bar:4.770   
                        baz:3     barz:3      baz:1 baz:3       baz:0  baz:0       
                                                                                   
Doc3001         3001    foo:11    foo:2.436   foo:0  foo:0      foo:1  foo:2.468   foo:2  foo:4.936
本来の値                bar:1001  bar:0.477   bar:5 bar:2.385   bar:10 bar:4.770   bar:20 bar:9.54 
                        baz:4     baz:2.875   baz:1 baz:2.875   baz:0  baz:0       baz:2  baz:5.750
                                                                                   
Doc3001         3000    foo:10    foo:2.477   foo:0  foo:0      foo:1  foo:2.477   foo:2  foo:4.954 
N,DF更新せず            bar:1000  bar:0.477   bar:5 bar:2.385   bar:10 bar:4.77    bar:20 bar:9.54 
                        baz:3     barz:3      baz:1 baz:3       baz:0  baz:0       baz:2  baz:6     


N,DFを更新しない（差分計算の）場合
Doc3001 のfoo2 が若干高めに出、baz2 が若干低めに出ている。
但し、他の既存ドキュメント（Doc1 , Doc3000）も同じ傾向。相対的な誤差は小さいとみて良さそう。

　　　TFIDFの実装の都合上、idfコレクションの.metaを参照するが
　　　この辺りを引数で修正してあげれば、スムーズに改修できそう。

