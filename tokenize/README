* About monmo-NLProcessing (TOKENIZE)
　　MongoDB上で動作する　”とにかくお手軽な　形態素解析エンジンです。

　　　MongoDBが立ち上がってる状態からなら、１０分程度で使える状態になります！！

　　MongoDBのコレクションを直接、形態素解析を掛ける事ができます。
　　結果は別のコレクションに保存するかJSON形式で標準出力に出力できます。

* 特徴
　　単純な辞書にマッチさせるだけでなく、ある程度文法や活用形を判断しているので
　　正確な分割や、品詞の抽出ができます。

　　　とはいえ、特に短い単語は難度が高く、混ざるケースもあります。（※下記既知の問題参照のこと）

　　MongoDB上に辞書を持ち、外来語や数値などを検出して、解析中にも辞書をアップデートしながら解析します。

* 解析精度
　　速度よりも手軽さと精度を重視した解析エンジンです。

　　速度を重視するならば、単純な先頭最大マッチが早いのですがそれでは精度が出ません。
　　特にひらがなの部分は壊滅的です。

　　また速度重視のエンジンは巷に溢れているのでわざわざ作る必要もありませんしね。


　　通常２深度以上の探索を行います。
　　　１．直前の品詞により、品詞の候補を絞る。
　　　２．先頭最大マッチをかける。
　　　３．その次に来る品詞や単語がマッチするなら採用。駄目なら２．に戻る。
　　　４．３．と同様、更に次を評価し続ける。助詞、助動詞、名詞、動詞の打ち切りなどで、１．に戻る。

　　　本来はドキュメント全体を評価し終えるまで３．を続けるべきだが、その様な探索アルゴリズムは複雑で重くなる。
　　　軽量化の為、適当なところで探索を打ち切る必要があり、こうなっている。


　　例： |仕事|が|大方|終わっ|た|

      1. 文章の先頭は助詞、助動詞以外
　　　　 2. 最大マッチ　　　　　　　　　　　　　　　　　=> 仕事　　：（名詞）
　　　　　 3. 名詞の後は基本助詞がくる。　　　　　　　　=> が　　　：（助詞）
      1. 助詞の後は殆ど何でもあり。
　　　 2. 最大マッチ　　　　　　　　　　　　　　　　　　=> 大方　　：（名詞＆副詞可能）
　　　　 3. 副詞可能の後は動詞が来る場合がある。　　　　=> 終わっ　：（動詞）『終わる』連用形
　　　　　 4. 動詞の連用形は『っ』は助詞、助動詞が必須。=> た　　　：（助動詞）『た』原型
　　　　　
* クイックスタート
　　０．MONMO環境を構築
				
　　　　　https://github.com/monmo/monmo/blob/master/README

　　１．IPA辞書をダウンロード

　　　　　pushd ./data
　　　　　wget http://iij.dl.sourceforge.jp/ipadic/24435/ipadic-2.7.0.tar.gz
　　　　　tar xzf ipadic-2.7.0.tar.gz
　　　　　popd

　　２．辞書をビルド（デフォルト辞書コレクション＝"analysis.dictionary"

　　　　　./gendic.sh -i data/ipadic-2.7.0

　　３．動作確認（形態素解析結果が出力されます

　　　　　./test.jp.sh -i "世界の皆さんこんにちは。" 

　　４．テストデータをMongoDBに投入

　　　　　mongoimport --drop -d test -c sampledoc --file ../sample/data/sampledocs.json

　　５．複数ドキュメントを一気に解析

　　　　　./tokenizer.jp.sh -s test.sampledoc -f body -o test.token.sampledoc


　　６．結果を確認

　　　　　mongo test <<<'db.token.sampledoc.find().sort({docid:1,idx:1})'


　　７．キーワード検索


　　　　『ペーパー』を含むドキュメント：

　　　　　./keyword_search.sh -s test.token.sampledoc -w 'ペーパー' -V 

　　　　『ペーパー』から始まる単語を含むドキュメント：

　　　　　./keyword_search.sh -s test.token.sampledoc -w 'ペーパー' -F -V 


　　８．熟語解析

　　　　　./phrase.sh -s test.token.sampledoc


　　９．熟語解析結果


　　　　cvフィールドが大きい程、熟語の確率が高い：

　　　　　mongo test <<<'db.phrase.token.sampledoc.find({cv:{$gt:0}}).sort({cv:-1})'



* 詳細情報

** 辞書詳細：( defalut: analysis.dictionary )

　　　　辞書はIPADICを元に同字意義語や活用形などを考慮した状態に直して使います。

　　辞書コレクションの構造

　　　　w: 単語 or 活用候補配列
　　　　　　　　単語か活用候補


　　　　h: 先頭文字配列
　　　　　　　　最長マッチにかける為のインデックス用の要素

　　　　l: 文字列長　　　　
　　　　　　　　最長マッチにかける為のインデックス用の要素

　　　　s: 品詞優先度
　　　　　　　　最長マッチにかける為のインデックス用の要素

　　　　c: コスト
　　　　　　　　未使用（品詞で別けた方が結果が良かったので使わなくなった）

　　　　f: 活用系区分
　　　　　　　　1: 打ち切り可能
　　　　　　　　2: 後ろ助詞、助動詞必須
　　　　　　　　3: 後ろ動詞可能（通常は動詞＋動詞は無い）
　　　　　　　　4: 後ろ名詞優先

　　　　　※ この辺は今後チューニングが必要

　　インデックス：

　　　　{h:1,l:-1,s:1}
　　　　{w:1}
　　　　{t:1}

** 結果コレクション

　　形態素解析の結果であると共に、Vectorize元データになっています。

　　　　docidとcを集計するとDF,TFが得られます。

　　結果コレクションの構造

　　　　docid: 元ドキュメントの_id

　　　　idx: 単語の現れた順

　　　　pos: ドキュメント中の単語の位置

　　　　w: 単語

　　　　l: 単語長
　　　　　　活用している場合は同じ単語でも長さが変わります

　　　　c: 単語ID（辞書コレクション中の_id）


　　インデックス：

　　　　{docid:1,idx:1}


　　辞書ビルド：
　　　　
　　　　gendic.sh の --nheads オプションは単語のインデックスに含める先頭文字数です。（デフォルト２）
　　　　　この値は辞書コレクションの".meta"レコードに保存され、解析時にも使われます。

　　　　nheadsを大きくすると、インデックスサイズが大きくなり、クエリー回数が増え、比較回数が減ります。


　　　　　例えばnheads=2の場合『インデックス』という単語では{ h: ["イ","イン"],l:6,s:300} この様にインデックスを張ります。

　　　　　　解析時に最初に以下のクエリーを試します。


　　　　　『イン』で始まる単語を長い順：
　　　　　　　db.dictionary.find({h:"イン"}).sort({l:-1,s:1})
　　　　　　　　- インフォメーション　　　　　　　
　　　　　　　　- インターナショナル
　　　　　　　　- インフォメーション　　　　　　　
　　　　　　　　- インターネット
　　　　　　　　- インデックス
　　　　　　　　- インデアン

　　　　　　　この場合、インデックスにマッチさせる為には５回の比較が必要です。
　　　　　　　　　　　　
　　　　　　ところがnheads=3ではこうなります。

　　　　　『インデ』で始まる単語を長い順：
　　　　　　　db.dictionary.find({h:"インデ"}).sort({l:-1,s:1})
　　　　　　　　- インデックス
　　　　　　　　- インデアン

　　　　　　　この場合、１回の比較でマッチできます。


　　　　nheadsを４以上にしても効果は薄くデメリットが目立つようになるようです。

　　　　事実上、nheadsは２or３なのですが、２＝＞３ではクエリー回数は約１．５倍。比較回数（＝辞書からフェッチする単語数）は約１／４。
　　　　データ転送量が４倍違うにも関わらず、性能はほぼ同じか２の方が良い事が多いのです。


** 既知の問題
　MONMORPは通常、実用上問題ない程度の精度で解析できますが
　幾つか間違った分割をしてしまうケースが判明しています。

　１．動詞＋名詞のパターンで判定を間違える場合があります。（ほぼ解決済み）

　　　- これを買うとしあわせに、、
　　　- これを買うときは、、



　　　- これ|を|買う|と|しあわせ|に|、、
　　　- これ|を|買う|とき|は|、、

　　　正しくはこの様に分割すべきですが、MONMORPでは以下の様に分割してしまいます。

　　　- これ|を|買う|と|しあわせ|に|、、
　　　- これ|を|買う|と|き|は|、、

　　　これは動詞の活用形で、後ろに助詞、助動詞、動詞などが続く可能性がある場合、助詞を優先するようになっているからです。
　　　名詞を優先したり、最長マッチを採用してしまうと

　　　- これ|を|買う|とし|あわせ|に|、、

　　　この様な結果を得てしまい、難しく解決できていません。
　　　これを解くには、もう少し深い探索を実装する必要があるのですが、性能との兼ね合いもあるので保留しています。


　２．外来語がサ変接続する場合

　　　- ビルドする、、

　　　- ビルド|する|、、
　　　正しくはこの様に分割すべきですが、MONMORPでは以下の様に分割してしまいます。

　　　- ビルド|す|る|、、

　　　MONMORPは連続したアルファベット、数字、カタカナを一般名詞として扱います。
　　　名詞の直後には基本的に動詞は来ないのですが（『アメリカ』する。など）、例外的に『する』に接続する場合があります。
　　　特に『する』はサ行変格活用で厄介な活用をする動詞で、助詞などと誤判定し易く、現状助ける手立てがありません。

　　　どうしてもこのパターンを拾いたい場合は辞書を編集し、この様な単語を登録する必要があります。
　　　　{w:"ビルド", h:["ビ","ビル"] l:3 , c:0, s:0 , t:["名詞","サ変接続"]}


　　　辞書の編集機能はまだ開発中ですが、mongo shellからはこの様に操作します。

　　　　load('../lib/utils.js')
　　　　load('lib/morpho.js')
　　　　var nheads = db.dictionary.findOne({w:'.meta'}).nheads
　　　　var word = {w:'ビルド',l:3,c:0,s:300,t:['名詞','ORG','外来語','サ変接続']}
　　　　var dicword = morpho.forms(nheads,word)
　　		db.dictionary.findAndModify({ query: {w:dicword.w}, update: dicword, upsert: true, new:true});
　
* TODO

　　熟語解析結果の利用方法を検討
　　　- 辞書更新の必要性
　　　　　→　辞書更新すると、次から熟語を単語として抽出するようになる。
　　　　　『宇宙』『開発』『競争』は『宇宙開発競争』になり、『宇宙』でも『開発』でも『競争』出もなくなる。
　　　　　これはC-VALUE評価で解決できるが、ノイズとベクトルの肥大化が問題となる。

　　White space tokenizerを実装
　　　　英語を解析する場合に必要


　　正規化
　　　　辞書を弄る事で実現できそう。（活用形と同じ扱いでいけるか？）


　　ＡＳＣＩＩ正規化
　　　　現在は全角に寄せているが、本来は半角に寄せて、英語解析と互換した方がいい。

